{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Scikit-learn Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()   # raw data of type Bunch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TIP**: The Iris flower dataset or Fisher’s Iris dataset is a multivariate dataset introduced by the British statistician and biologist Ronald Fisher. The dataset consists of\n",
    "50 samples from each of three species of Iris (Iris setosa, Iris virginica, and Iris versicolor). Four features were measured from each sample: the length and the width\n",
    "of the sepals and petals in centimeters. Based on the combination of these four\n",
    "features, Fisher developed a linear discriminant model to distinguish the species\n",
    "from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.DESCR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.data)               # Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.feature_names)      # Feature Names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.target)             # Labels\n",
    "print(iris.target_names)       # Label names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(iris.data)   # convert features\n",
    "                               # to dataframe in Pandas\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data on breast cancer\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "\n",
    "# data on diabetes\n",
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "# dataset of 1797 8x8 images of hand-written digits\n",
    "digits = datasets.load_digits()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on the Scikit-learn dataset, check out the documentation at http://scikit-learn.org/stable/datasets/index.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Dataset\n",
    "\n",
    "Kaggle is the world’s largest community of data scientists and machine learners.\n",
    "What started off as a platform for offering machine learning competitions, Kaggle\n",
    "now also offers a public data platform, as well as a cloud-based workbench for\n",
    "data scientists. Google acquired Kaggle in March 2017.\n",
    "\n",
    "For learners of machine learning, you can make use of the sample datasets\n",
    "provided by Kaggle at https://www.kaggle.com/datasets/. Some of the interesting datasets include:\n",
    "\n",
    "- Women’s Shoe Prices: A list of 10,000 women’s shoes and the prices at which they are sold (https://www.kaggle.com/datafiniti/womensshoes-prices)\n",
    "\n",
    "- Fall Detection Data from China: Activity of elderly patients along with their medical information (https://www.kaggle.com/pitasr/falldata)\n",
    "\n",
    "- NYC Property Sales: A year’s worth of properties sold on the NYC real\n",
    "estate market (https://www.kaggle.com/new-york-city/nyc-propertysales#nyc-rolling-sales.csv)\n",
    "\n",
    "- US Flight Delay: Flight Delays for year 2016 (https://www.kaggle.com/niranjan0272/us-flight-delay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Your Own Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linearly Distributed Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `make_regression()` function generates data that is linearly distributed.\n",
    "You can specify the number of features that you want, as well as the standard\n",
    "deviation of the Gaussian noise applied to the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets.samples_generator import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=5.4)\n",
    "plt.scatter(X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustered Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `make_blobs()` function generates n number of clusters of random data. This\n",
    "is very useful when performing clustering in unsupervised learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "X, y = make_blobs(500, centers=3)  # Generate isotropic Gaussian\n",
    "                                   # blobs for clustering\n",
    "\n",
    "rgb = np.array(['r', 'g', 'b'])\n",
    "\n",
    "# plot the blobs using a scatter plot and use color coding\n",
    "plt.scatter(X[:, 0], X[:, 1], color=rgb[y])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustered Dataset Distributed in Circular Fashion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `make_circles()` function generates a random dataset containing a large circle\n",
    "embedding a smaller circle in two dimensions. This is useful when performing\n",
    "classifications, using algorithms like SVM (Support Vector Machines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, y = make_circles(n_samples=100, noise=0.09)\n",
    "\n",
    "rgb = np.array(['r', 'g', 'b'])\n",
    "plt.scatter(X[:, 0], X[:, 1], color=rgb[y])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to get started with machine learning with Scikit-learn is to start\n",
    "with linear regression. *Linear regression* is a linear approach for modeling the\n",
    "relationship between a scalar dependent variable y and one or more explanatory variables (or independent variables). For example, imagine that you have\n",
    "a set of data comprising the heights (in meters) of a group of people and their\n",
    "corresponding weights (in kg):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# represents the heights of a group of people in metres\n",
    "heights = [[1.6], [1.65], [1.7], [1.73], [1.8]]\n",
    "\n",
    "# represents the weights of a group of people in kgs\n",
    "weights = [[60], [65], [72.3], [75], [80]]\n",
    "\n",
    "plt.title('Weights plotted against heights')\n",
    "plt.xlabel('Heights in metres')\n",
    "plt.ylabel('Weights in kilograms')\n",
    "\n",
    "plt.plot(heights, weights, 'k.')\n",
    "\n",
    "# axis range for x and y\n",
    "plt.axis([1.5, 1.85, 50, 90])\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TIP**: Observe that the *heights* and *weights* are both represented as\n",
    "two-dimensional lists. This is because the `fit()` function requires both the X and y\n",
    "arguments to be two-dimensional (of type `list` or `ndarray`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the LinearRegression Class for Fitting the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we draw the straight line that cuts though all of the points? It turns\n",
    "out that the Scikit-learn library has the `LinearRegression` class that helps you\n",
    "to do just that. All you need to do is to create an instance of this class and use the *heights* and *weights* lists to create a linear regression model using the `fit()`\n",
    "function, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create and fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X=heights, y=weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction\n",
    "weight = model.predict([[1.75]])[0][0]\n",
    "print(round(weight,2))         # 76.04\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TIP**: In Scikit-learn, you typically use the `fit()` function to train a model. Once the\n",
    "model is trained, you use the `predict()` function to make a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Linear Regression Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be useful to visualize the linear regression line that has been created\n",
    "by the *LinearRegression* class. Let’s do this by first plotting the original data\n",
    "points and then sending the *heights* list to the model to predict the weights.\n",
    "We then plot the series of forecasted weights to obtain the line. The following\n",
    "code snippet shows how this is done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "heights = [[1.6], [1.65], [1.7], [1.73], [1.8]]\n",
    "weights = [[60], [65], [72.3], [75], [80]]\n",
    "\n",
    "plt.title('Weights plotted against heights')\n",
    "plt.xlabel('Heights in metres')\n",
    "plt.ylabel('Weights in kilograms')\n",
    "\n",
    "plt.plot(heights, weights, 'k.')\n",
    "\n",
    "plt.axis([1.5, 1.85, 50, 90])\n",
    "plt.grid(True)\n",
    "\n",
    "# plot the regression line\n",
    "plt.plot(heights, model.predict(heights), color='r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Gradient and Intercept of the Linear Regression Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is not clear at what value the linear regression line intercepts\n",
    "the y-axis. This is because we have adjusted the x-axis to start plotting at 1.5. A\n",
    "better way to visualize this would be to set the x-axis to start from 0 and enlarge\n",
    "the range of the y-axis. You then plot the line by feeding in two extreme values\n",
    "of the height: 0 and 1.8. The following code snippet re-plots the points and the\n",
    "linear regression line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Weights plotted against heights')\n",
    "plt.xlabel('Heights in metres')\n",
    "plt.ylabel('Weights in kilograms')\n",
    "\n",
    "plt.plot(heights, weights, 'k.')\n",
    "\n",
    "plt.axis([0, 1.85, -200, 200])\n",
    "plt.grid(True)\n",
    "\n",
    "# plot the regression line\n",
    "extreme_heights = [[0], [1.8]]\n",
    "plt.plot(extreme_heights, model.predict(extreme_heights), color='b')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While you can get the y-intercept by predicting the weight if the height is 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(model.predict([[0]])[0][0],2)   # -104.75\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the model object provides the answer directly through the intercept_ property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(model.intercept_[0],2))   # -104.75\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the model object, you can also get the gradient of the linear regression\n",
    "line through the coef_ property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(model.coef_[0][0],2))     # 103.31\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the Performance of the Model by Calculating the Residual Sum of Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know if your linear regression line is well fitted to all of the data points, we\n",
    "use the *Residual Sum of Squares* (RSS) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print('Residual sum of squares: %.2f' %\n",
    "       np.sum((weights - model.predict(heights)) ** 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RSS should be as small as possible, with 0 indicating that the regression line fits the points exactly (rarely achievable in the real world)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model Using a Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "heights_test = [[1.58], [1.62], [1.69], [1.76], [1.82]]\n",
    "weights_test = [[58], [63], [72], [73], [85]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Sum of Squares (TSS)\n",
    "weights_test_mean = np.mean(np.ravel(weights_test))\n",
    "TSS = np.sum((np.ravel(weights_test) -\n",
    "              weights_test_mean) ** 2)\n",
    "print(\"TSS: %.2f\" % TSS)\n",
    "\n",
    "# Residual Sum of Squares (RSS)\n",
    "RSS = np.sum((np.ravel(weights_test) -\n",
    "              np.ravel(model.predict(heights_test)))\n",
    "                 ** 2)\n",
    "print(\"RSS: %.2f\" % RSS)\n",
    "\n",
    "# R_squared\n",
    "R_squared = 1 - (RSS / TSS)\n",
    "print(\"R-squared: %.2f\" % R_squared)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TIP**: The ravel() function converts the two-dimensional list into a contiguous\n",
    "flattened (one-dimensional) array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, you don’t have to calculate the R-Squared manually yourself—\n",
    "Scikit-learn has the score() function to calculate the R-Squared automatically\n",
    "for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using scikit-learn to calculate r-squared\n",
    "print('R-squared: %.4f' % model.score(heights_test,\n",
    "                                      weights_test))\n",
    "\n",
    "# R-squared: 0.9429\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An R-Squared value of 0.9429 (94.29%) indicates a pretty good fit for your test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting the Model\n",
    "\n",
    "Once you have trained a model, it is often useful to be able to save it for later\n",
    "use. Rather than retraining the model every time you have new data to test, a\n",
    "saved model allows you to load the trained model and make predictions immediately without the need to train the model again.\n",
    "\n",
    "There are two ways to save your trained model in Python:\n",
    "- Using the standard pickle module in Python to serialize and deserialize objects\n",
    "- Using the joblib module in Scikit-learn that is optimized to save and load Python objects that deal with NumPy data\n",
    "\n",
    "The first example you will see is saving the model using the pickle module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'HeightsAndWeights_model.sav'\n",
    "# write to the file using write and binary mode\n",
    "pickle.dump(model, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding code snippet, you first opened a file in \"wb\" mode (\"w\" for\n",
    "write and \"b\" for binary). You then use the dump() function from the pickle\n",
    "module to save the model into the file. \n",
    "\n",
    "To load the model from file, use the load() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = loaded_model.score(heights_test,\n",
    "                            weights_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the joblib module is very similar to using the pickle module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'HeightsAndWeights_model2.sav'\n",
    "joblib.dump(model, filename)\n",
    "\n",
    "# load the model from disk\n",
    "loaded_model = joblib.load(filename)\n",
    "result = loaded_model.score(heights_test,\n",
    "                            weights_test)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Rows with NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://drive.google.com/uc?id=1YXCQx-K7o4ITIRl0TX9L_4VtQGtPnxSs')\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing NaN with the Mean of the Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all the NaNs in column B with the average of column B\n",
    "df.B = df.B.fillna(df.B.mean())\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://drive.google.com/uc?id=1YXCQx-K7o4ITIRl0TX9L_4VtQGtPnxSs')\n",
    "df = df.dropna()                             # drop all rows with NaN\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that after removing the rows containing NaN, the index is no longer in\n",
    "sequential order. If you need to reset the index, use the reset_index() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)               # reset the index\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Duplicate Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://drive.google.com/uc?id=16xLZ4HBsf6WK5TJg5MuLx30lUMETln9W')\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "print(df.duplicated(keep=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keep argument allows you to specify how to indicate\n",
    "duplicates:\n",
    "- The default is 'first': All duplicates are marked as True except for the\n",
    "first occurrence\n",
    "- 'last': All duplicates are marked as True except for the last occurrence\n",
    "- False: All duplicates are marked as True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.duplicated(keep=\"first\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df.duplicated(keep=False)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(keep='first', inplace=True)  # remove duplicates and keep the first\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TIP**: By default, the drop_duplicates() function will not modify the original\n",
    "dataframe and will return the dataframe containing the dropped rows. If you want to\n",
    "modify the original dataframe, set the inplace parameter to True, as shown in the\n",
    "preceding code snippet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, you only want to remove duplicates that are found in certain\n",
    "columns in the dataset. For example, if you look at the dataset that we have been using, observe that for row 3 and row 4, the values of column A and C are identical. You can remove duplicates in certain columns by specifying the subset\n",
    "parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['A', 'C'], keep='last',\n",
    "                           inplace=True)     # remove all duplicates in\n",
    "                                             # columns A and C and keep\n",
    "                                             # the last\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TIP**: To remove all duplicates, set the keep parameter to False. To keep the last\n",
    "occurrence of duplicate rows, set the keep parameter to 'last'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization is crucial for some algorithms to model the data correctly. For\n",
    "example, one of the columns in your dataset may contain values from 0 to 1,\n",
    "while another column has values ranging from 400,000 to 500,000. The huge\n",
    "disparity in the scale of the numbers could introduce problems when you use\n",
    "the two columns to train your model. Using normalization, you could maintain the ratio of the values in the two columns while keeping them to a limited\n",
    "range. In Pandas, you can use the MinMaxScaler class to scale each column to\n",
    "a particular range of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "df = pd.read_csv('https://drive.google.com/uc?id=1bSUZBtBV37KAv9f2LAw5AHV3QZzLYgQU')\n",
    "print(df)\n",
    "\n",
    "x = df.values.astype(float)\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df = pd.DataFrame(x_scaled, columns=df.columns)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics, an outlier is a point that is distant from other observed points.\n",
    "For example, given a set of values—234, 267, 1, 200, 245, 300, 199, 250, 8999, and\n",
    "245—it is quite obvious that 1 and 8999 are outliers. They distinctly stand out\n",
    "from the rest of the values, and they “lie outside” most of the other values in the\n",
    "dataset; hence the word outlier. Outliers occur mainly due to errors in recording\n",
    "or experimental error, and in machine learning it is important to remove them\n",
    "prior to training your model as it may potentially distort your model if you don’t.\n",
    "There are a number of techniques to remove outliers, and in this chapter we\n",
    "discuss two of them:\n",
    "- Tukey Fences\n",
    "- Z-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tukey Fences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tukey Fences is based on Interquartile Range (IQR). IQR is the difference between\n",
    "the first and third quartiles of a set of values. The first quartile, denoted Q1,\n",
    "is the value in the dataset that holds 25% of the values below it. The third quartile, denoted Q3, is the value in the dataset that holds 25% of the values above it. Hence, by definition, IQR = Q3 – Q1.\n",
    "\n",
    "In Tukey Fences, outliers are values that are as follows:\n",
    "- Less than Q1 – (1.5 × IQR), or\n",
    "- More than Q3 + (1.5 × IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def outliers_iqr(data):\n",
    "    q1, q3 = np.percentile(data, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - (iqr * 1.5)\n",
    "    upper_bound = q3 + (iqr * 1.5)\n",
    "    return np.where((data > upper_bound) | (data < lower_bound))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TIP**: The np.where() function returns the location of items satisfying the conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the Tukey Fences, let’s use the famous Galton dataset on the heights\n",
    "of parents and their children. The dataset contains data based on the famous\n",
    "1885 study of Francis Galton exploring the relationship between the heights of\n",
    "adult children and the heights of their parents. Each case is an adult child, and\n",
    "the variables are as follows:\n",
    "- Family: The family that the child belongs to, labeled by the numbers from 1 to 204 and 136A\n",
    "- Father: The father’s height, in inches\n",
    "- Mother: The mother’s height, in inches\n",
    "- Gender: The gender of the child, male (M) or female (F)\n",
    "- Height: The height of the child, in inches\n",
    "- Kids: The number of kids in the family of the child\n",
    "\n",
    "The dataset has 898 cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"http://www.mosaic-web.org/go/datasets/galton.csv\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Outliers using outliers_iqr()\")\n",
    "print(\"=============================\")\n",
    "for i in outliers_iqr(df.height)[0]:\n",
    "    print(df[i:i+1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second method for determining outliers is to use the Z-score method. A\n",
    "Z-score indicates how many standard deviations a data point is from the mean.\n",
    "The Z-score has the following formula: \n",
    "\n",
    "$$Z =\\frac{(x_i - \\mu)}{\\sigma}$$\n",
    "\n",
    "where $x_i$ is the data point, $\\mu$ is the mean of the dataset, and $\\sigma$ is the standard\n",
    "deviation.\n",
    "This is how you interpret the Z-score:\n",
    "- A negative Z-score indicates that the data point is less than the mean, and\n",
    "a positive Z-score indicates the data point in question is larger than\n",
    "the mean\n",
    "- A Z-score of 0 tells you that the data point is right in the middle (mean),\n",
    "and a Z-score of 1 tells you that your data point is 1 standard deviation\n",
    "above the mean, and so on\n",
    "- Any Z-score greater than 3 or less than –3 is considered to be an outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers_z_score(data):\n",
    "    threshold = 3\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    z_scores = [(y - mean) / std for y in data]\n",
    "    return np.where(np.abs(z_scores) > threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Outliers using outliers_z_score()\")\n",
    "print(\"=================================\")\n",
    "for i in outliers_z_score(df.height)[0]:\n",
    "    print(df[i:i+1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
